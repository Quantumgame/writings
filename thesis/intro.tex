\chapter{Introduction}

The nature of information encoded by auditory networks in the brain has been described by a variety of experimental approaches that vary in their choice of stimuli, stimulus representation, and predictive modelling approach. Neurons in the auditory system have been probed with simple stimuli such as tones, as well as natural stimuli that tend to elicit more robust neural responses. There is a spectrum of stimulus paradigms and stimulus-response models that can be constructed to better understand the relationship between the properties of sound and the spiking of auditory neurons. These models depend in large part on the richness of the stimulus and the numerical representation used to describe it. At the simple end of the spectrum are artificial pure tones, which can be quantified completely by their amplitude and frequency. Neuron response properties have been described using tuning curves that predict spike rate from the amplitude and frequency of simple tone stimuli. These models have been used with some success to describe neuronal response properties in early auditory areas \cite{REF}, and even to describe tonotopy in human auditory cortex \cite{REF}.
However, natural stimuli are not simply described by their amplitude and frequency. Human speech is a variable and complex sequence of smoothly changing harmonic stacks and noisy bursts \cite{REF}. Some bird vocalizations share a similar complexity; Zebra Finch songs are complex but rigid sequence of harmonic stacks, noise bursts, and chirps \cite{REF}. The need to utilize natural sound stimuli to more effectively probe neuron responses necessitates a stimulus representation more complex than amplitude and frequency alone. Complete information about the time-varying acoustic features of a sound can be quantified using a spectrogram. Spectrograms represent the sound as a set of frequencies that vary over time, and can be inverted to produce the original sound pressure waveform \cite{Cohen1995}. To accommodate this complex representation, spatio-temporal receptive fields (STRFs) can be fit to describe neuronal responses to natural sounds using a weighted sum of the recent spectrogram history \cite{REF}. STRFs have been used with much success to describe auditory neurons both in mammals \cite{REF} and birds \cite{REF}. Notably, tonotopy has not been observed in higher Avian auditory areas; in it’s place is a “STRFotopy”, where temporal memory and spectral bandwidth of neuronal responses vary over anatomical space \cite{Kim2011}.
At an intermediate level of representation, natural sounds that are short and isolated in time can be represented by a small set of summary statistics that intuitively describe how they vary spectrally, temporally, and spectro-temporally \cite{REF}. This approach has been utilized to successfully classify the behavioral context and semantic meaning of Zebra finch vocalizations \cite{Elie2015b}. We chose the acoustic feature set utilized by \cite{Elie2015b} to represent Zebra finch vocalizations, and described the relationship between these acoustic features and neuron activity. These stimulus-response models describe neuronal activity as a function not just of amplitude and frequency, but a richer set of features closer to perceptual properties such as pitch and spectral or temporal noisiness.
It is unlikely that a clear understanding of the brain can be obtained by the study of single neurons alone. The successful adoption and use of multi-electrode arrays allows Neuroscientists to simultaneously record from many neurons distributed over a large area. By utilizing data from ensembles of auditory neurons presented with natural sounds, we can develop insight into how neurons work together as a population to represent stimulus information. A core observation that sets the context for understanding population coding is that neurons integrate input from many other neurons, and temporally coincident input from multiple input neurons drives stronger spiking activity than non-coincident input \cite{REF}. This implies that stimulus information may be encoded and transmitted not only by the idiosyncratic firing of individual neurons, but in addition by the temporal correlations of network firing patterns. Approaches to understand the population code at this level have utilized Information Theory to quantify the amount of stimulus information contained in an ensemble of neurons, as well as Machine Learning approaches to directly decode stimulus features \cite{Quiroga2009}. These approaches have led to a deeper understanding of the population code.
There is evidence that neurons exhibit robust spatial correlations in their spike patterns. Analysis of retinal ganglion cell activity by \cite{Schlens2006} and \cite{Schneidman2006}, using random flicker and white noise visual stimuli, respectively, demonstrated the existence of significant pairwise correlations between firing neurons. \cite{Denman2014} showed pairwise correlations between V1 neurons using sinusoidal grating stimuli. In the auditory system, \cite{Hamilton2013} showed that pairwise connectivity between neurons in mouse auditory cortex could be modulated by optogenetic activation of inhibitory interneurons. However, the existence of correlated activity does not imply that correlations actually carry stimulus information. Information theoretic frameworks have been constructed to analyze the stimulus information carried by ensembles of neurons independently by their spike rates, and in addition their correlations (\cite{Panzeri2001}, \cite{Schneidman2003}, \cite{Nirenberg2003}). Complementary decoding approaches can be used to disentangle the effects of redundancy and synergy in correlations. Using a decoding approach, \cite{Ince2013} show in monkey auditory cortex that the ensemble spike rates of neurons contain non-redundant information about sound stimuli, and decoding performance increases with the number of neurons considered. They found that there is a small group of neurons that contain most of the stimulus information. Following up with an information theoretic approach, they found that correlations in neural activity do not contain stimulus information. In this work we show that including correlated spiking activity in addition to population spike rate improves the performance of decoders trained to predict acoustic features.
    Electrical activity from the synapses and membrane currents of many neurons contributes to an aggregate signal known as the local field potential (LFP). The LFP is a complex mixture of synaptic and transmembrane currents elicited by sodium and calcium spikes \cite{Buzsaki2012b}, and the biophysical origin of LFP power may vary by frequency \cite{Reimann2013}. Many studies show that the mammalian LFP oscillates at several different frequency bands. Very low frequency (< 2Hz) “slow” oscillations, observed during sleep and some types of anesthesia, may originate from the interplay of bursting neurons in the Thalamic Reticular Nucleus and cortex \cite{Lewis2015}. Oscillations in the range of 30-90Hz are typically labeled as “Gamma” oscillations. The neural mechanism of Gamma oscillations is thought to involve the spatial and temporal interplay between excitatory and inhibitory networks \cite{Buzsaki2012a}. Activity in different frequency bands is not mutually exclusive; lower frequency Theta oscillations (~7Hz) can modulate higher frequency Gamma oscillations in the Hippocampus in a manner that may help encode ordered sequence of items \cite{Lisman2013}. A nested hierarchy of frequency bands has been identified in auditory cortex of monkey that controls the excitability of neural activity and may optimize the auditory system for the processing of rhythmic vocalizations \cite{Lakatos2005}. In contrast to the well studied oscillations of mammalian cortex, there have not been many studies of LFP oscillations in the Avian brain. Analysis of multielectrode LFP was used by \cite{Beckers2014} to show three dimensional propagation of slow wave oscillations in Zebra finch forebrain, but higher frequencies were not studied, and they did not link this activity to sensory stimuli. In this work, we study LFP power in the 15-190Hz range of the Zebra finch auditory system, and show that the power spectrum can be used to decode acoustic features from the full repertoire of natural Zebra finch vocalizations. We then show that LFP power at each frequency can be predicted in large part from the population spike activity and zero-lag correlations of spiking of neurons in the auditory network.

